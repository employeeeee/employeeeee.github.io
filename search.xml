<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Tomcat路径访问去掉项目名称]]></title>
    <url>%2F2019%2F03%2F25%2FTomcat%E8%B7%AF%E5%BE%84%E8%AE%BF%E9%97%AE%E5%8E%BB%E6%8E%89%E9%A1%B9%E7%9B%AE%E5%90%8D%E7%A7%B0%2F</url>
    <content type="text"><![CDATA[部署项目的时候 发现了一个问题 访问项目的时候 需要加上项目的名称才可以访问这样就很不符合项目上线的实际应用场景.于是查了相关资料 解决了一下url访问去掉项目名的问题做一个记录 需要在server.xml配置文件中的&lt;Host&gt;标签中 添加&lt;context&gt;标签1&lt;Context docBase=&quot;项目的绝对路径&quot; path=&quot;&quot; reloadable=&quot;true&quot;&gt; 然后重启tomcat 查看日志 发现项目启动了两次 并且内存溢出 考虑解决办法 增大tomcat的运行内存 解决项目启动两次的问题 让项目只启动一次 第一种方法 在catalina.sh的配置文件中 添加 JAVA_OPTS=&quot;-Xms 1024m -Xmx2048m=XX:PermSize=256m -XX:MaxPermSize&quot; 但是并没有好用 我们的这个项目中很多没有用的jar包和代码都没有进行整理和优化 正常启动就需要几分钟 所以最后选择放弃这种办法让项目只启动一次 第二种解决办法 就是修改server.xml中的&lt;Host&gt;标签将appBase的内容改为appBase=&#39;&#39; 重启tomcat 问题解决]]></content>
      <categories>
        <category>tomcat</category>
      </categories>
      <tags>
        <tag>项目配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase相关]]></title>
    <url>%2F2019%2F03%2F12%2FHBase%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[NoSQL 数据库: HBaseNoSQL 数据库简介 not only sql2 常见的nosql数据库 HBase Redis: 基于内存的NoSQL数据库支持持久化: RDB AOF MongDB 基于文档型(BSON文档) 的NoSQL数据库 Cassandra: 跟HBase相似 HBase的体系架构(主从结构)和 表结构1 Hadoop的生态体系2 HBASE 基于HDFS智商的NoSQL数据库HBASE —-&gt; HDFS 表 目录 数据 文件默认大小 128M HBASE 的体系架构 搭建HBase的环境 解压hbase vi ~/.bash_profile HBASE_HOME= 路径 export HBASE_HOME PATH=$HBASE_HOME/bin:$PATH export PATH 2、伪分布模式：一个ZK、一个HMaster、一个RegionServer12345678910111213141516171819202122232425262728hbase-env.sh 129 export HBASE_MANAGES_ZK=true 使用HBase自带的ZooKeeperhbase-site.xml &lt;!--HBase对应的HDFS目录--&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://192.168.157.111:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;!--是一个分布式环境--&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--指定ZK的地址--&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;192.168.157.111&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; regionservers 操作HBase Web console1601060010(旧版本) 创建表： create ‘student’,’info’,’grade’ 查看表: list MySQL: show tables; Oracle: select * from tab; 查看表结构：desc &apos;student&apos; describe &apos;student&apos; SQL中（Oracle）：desc和describe什么区别？ 都是看表结构 (*) desc 是SQL*PLUS语句，可以缩写 (*) describe是SQL语句，不能缩写 （*）插入数据：put put &apos;student&apos;,&apos;stu003&apos;,&apos;info:age&apos;,&apos;13&apos; （*）查询数据： scan 相当于 select * from student get 相当于 select * from student where rowkey=??? get &apos;student&apos;,&apos;stu001&apos; 为了加快查询的速度，可以建立HBase的二级索引 （*）清空表数据：truncate 日志： hbase(main):005:0&gt; truncate &apos;student&apos; Truncating &apos;student&apos; table (it may take a while): - Disabling table... - Truncating table... 0 row(s) in 3.9740 seconds 老版本的HBase使用truncate 日志： hbase(main):005:0&gt; truncate &apos;student&apos; Truncating &apos;student&apos; table (it may take a while): - Disabling table... - Dropping table... - Creating table 0 row(s) in 3.9740 seconds 补充一个知识：delete和truncate什么区别？（以Oracle为例） 1、delete是DML（Data Manipulation Language）语句，DML可以回滚 truncate是DDL（Data Definition Language）语句，DDL不可以回滚 2、delete会产生碎片、truncate不会 3、delete不会释放空间，truncate会 4、delete可以闪回(flashback)，truncate不可以 （*）删除表： drop hbase(main):007:0&gt; disable &apos;student&apos; 0 row(s) in 2.2980 seconds hbase(main):008:0&gt; drop &apos;student&apos; 0 row(s) in 1.3770 seconds C:\Windows\System32\drivers\etc\hosts 里写入ip映射 192.168.28.136 master 数据保存的过程(Region的分裂)HBase的过滤器(相当于where语句)HBase的MapReduce]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce的实际操作]]></title>
    <url>%2F2019%2F03%2F06%2FMapReduce%E7%9A%84%E5%AE%9E%E9%99%85%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[MapReduce 求部门的工资总额 员工表SQL: select t.department,sum(wage) from emp_tables t group by t.department 分析数据处理的过程 MapReduce的高级特性 序列化 Java的序列化serializable 接口 MapReduce 的序列化: 核心接口 : Writeble 如果一个类实现了Writable的接口 该类的对象可以作为key和value 读取员工的数据 生成员工对象 直接输出到HDFS 使用MapReduce序列化 重写求部门工资总额的例子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129 package com.example.mapreducedemo.serializable;import org.apache.hadoop.io.Writable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;/** * @author employeeeee * @Descriotion: * 员工号 名字 职位 领导 入职日期 工资 奖金 部门号 * @date 2019/3/7 9:28 */public class Emp implements Writable &#123; private int empno; private String ename; private String job; private int mgr; private String hiredate; private int sal; private int comm; private int deptno; @Override public void write(DataOutput dataOutput) throws IOException &#123; //序列化 向输出流写对象 dataOutput.writeInt(this.empno); dataOutput.writeUTF(this.ename); dataOutput.writeUTF(this.job); dataOutput.writeInt(this.mgr); dataOutput.writeUTF(this.hiredate); dataOutput.writeInt(this.sal); dataOutput.writeInt(this.comm); dataOutput.writeInt(this.deptno); &#125; @Override public void readFields(DataInput dataInput) throws IOException &#123; //反序列化 从输入流读对象 this.empno = dataInput.readInt(); this.ename = dataInput.readUTF(); this.job = dataInput.readUTF(); this.mgr = dataInput.readInt(); this.hiredate = dataInput.readUTF(); this.sal = dataInput.readInt(); this.comm = dataInput.readInt(); this.deptno = dataInput.readInt(); &#125; @Override public String toString() &#123; return "员工信息&#123;" + "empno=" + empno + ", ename='" + ename + '\'' + ", job='" + job + '\'' + ", sal=" + sal + ", deptno=" + deptno + '&#125;'; &#125; public int getEmpno() &#123; return empno; &#125; public void setEmpno(int empno) &#123; this.empno = empno; &#125; public String getEname() &#123; return ename; &#125; public void setEname(String ename) &#123; this.ename = ename; &#125; public String getJob() &#123; return job; &#125; public void setJob(String job) &#123; this.job = job; &#125; public int getMgr() &#123; return mgr; &#125; public void setMgr(int mgr) &#123; this.mgr = mgr; &#125; public String getHiredate() &#123; return hiredate; &#125; public void setHiredate(String hiredate) &#123; this.hiredate = hiredate; &#125; public int getSal() &#123; return sal; &#125; public void setSal(int sal) &#123; this.sal = sal; &#125; public int getComm() &#123; return comm; &#125; public void setComm(int comm) &#123; this.comm = comm; &#125; public int getDeptno() &#123; return deptno; &#125; public void setDeptno(int deptno) &#123; this.deptno = deptno; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637 package com.example.mapreducedemo.serializable;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException;/** * @author employeeeee * @Descriotion: * @date 2019/3/7 9:56 */public class EmpInfoMapper extends Mapper&lt;LongWritable,Text, IntWritable,Emp&gt; &#123; @Override protected void map(LongWritable key1, Text value1, Context context) throws IOException, InterruptedException &#123; String a = value1.toString(); String[] words = a.split(","); //定义一个新的emp对象 并赋值 Emp emp = new Emp(); emp.setEmpno(Integer.parseInt(words[0])); emp.setEname(words[1]); emp.setJob(words[2]); emp.setMgr(Integer.parseInt(words[3])); emp.setHiredate(words[4]); emp.setSal(Integer.parseInt(words[5])); emp.setComm(Integer.parseInt(words[6])); emp.setDeptno(Integer.parseInt(words[7])); //输出 context.write(new IntWritable(emp.getEmpno()),emp); &#125;&#125; 排序 规则: 按照key2排序 基本数据类型 数字 默认:升序 可以改变默认的排序规则(创建自己的比较器) 123456789101112131415 package com.example.mapreducedemo.serializetotal;import org.apache.hadoop.io.IntWritable;/** * @author employeeeee * @Descriotion: * @date 2019/3/7 14:02 */public class MyComparator extends IntWritable.Comparator &#123; @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) &#123; return -super.compare(b1, s1, l1, b2, s2, l2); &#125;&#125; 1234567891011121314151617181920212223242526272829303132 package com.example.mapreducedemo.comparable.sort;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;/** * @author employeeeee * @Descriotion: * @date 2019/3/7 15:45 */public class EmpSortMain &#123; public static void main(String[] args) throws Exception &#123; Job job = Job.getInstance(new Configuration()); job.setJarByClass(EmpSortMain.class); job.setMapperClass(EmpSortMapper.class); job.setOutputKeyClass(EmpSort.class); job.setOutputValueClass(NullWritable.class); FileInputFormat.setInputPaths(job,new Path(args[0])); FileOutputFormat.setOutputPath(job,new Path(args[1])); job.waitForCompletion(true); &#125;&#125; 字符串 默认:字典顺序 对象 sql 的排序order by后面 + 列名 表达式 别名 序号oracle 排序的结果 是临时表的数据 不是原来的表 员工对象的排序 接口WritableComparable 一个列排序 多个列排序 有几个条件就写几个条件 把等号写到最后一个条件上.(前提) 该对象必须是Key2实现了hadoop序列化接口 对象是可排序的 类似 Java的对象排序 comparable 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147 package com.example.mapreducedemo.comparable.sort;import org.apache.hadoop.io.Writable;import org.apache.hadoop.io.WritableComparable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;/** * @author employeeeee * @Descriotion: * 员工号 名字 职位 领导 入职日期 工资 奖金 部门号 * @date 2019/3/7 9:28 */public class EmpSort implements WritableComparable&lt;EmpSort&gt; &#123; private int empno; private String ename; private String job; private int mgr; private String hiredate; private int sal; private int comm; private int deptno; @Override public int compareTo(EmpSort o) &#123; //定义自己的比较规则 //如果是多个列 那么有几个条件写几个条件,最后一个列加等号 if (this.deptno&gt;o.deptno)&#123; return 1; &#125;else if (this.deptno&lt;o.deptno)&#123; return -1; &#125; if (this.sal&gt;=o.sal)&#123; return 1; &#125;else &#123; return -1; &#125; &#125; @Override public void write(DataOutput dataOutput) throws IOException &#123; //序列化 向输出流写对象 dataOutput.writeInt(this.empno); dataOutput.writeUTF(this.ename); dataOutput.writeUTF(this.job); dataOutput.writeInt(this.mgr); dataOutput.writeUTF(this.hiredate); dataOutput.writeInt(this.sal); dataOutput.writeInt(this.comm); dataOutput.writeInt(this.deptno); &#125; @Override public void readFields(DataInput dataInput) throws IOException &#123; //反序列化 从输入流读对象 this.empno = dataInput.readInt(); this.ename = dataInput.readUTF(); this.job = dataInput.readUTF(); this.mgr = dataInput.readInt(); this.hiredate = dataInput.readUTF(); this.sal = dataInput.readInt(); this.comm = dataInput.readInt(); this.deptno = dataInput.readInt(); &#125; @Override public String toString() &#123; return "员工信息&#123;" + "empno=" + empno + ", ename='" + ename + '\'' + ", job='" + job + '\'' + ", sal=" + sal + ", deptno=" + deptno + '&#125;'; &#125; public int getEmpno() &#123; return empno; &#125; public void setEmpno(int empno) &#123; this.empno = empno; &#125; public String getEname() &#123; return ename; &#125; public void setEname(String ename) &#123; this.ename = ename; &#125; public String getJob() &#123; return job; &#125; public void setJob(String job) &#123; this.job = job; &#125; public int getMgr() &#123; return mgr; &#125; public void setMgr(int mgr) &#123; this.mgr = mgr; &#125; public String getHiredate() &#123; return hiredate; &#125; public void setHiredate(String hiredate) &#123; this.hiredate = hiredate; &#125; public int getSal() &#123; return sal; &#125; public void setSal(int sal) &#123; this.sal = sal; &#125; public int getComm() &#123; return comm; &#125; public void setComm(int comm) &#123; this.comm = comm; &#125; public int getDeptno() &#123; return deptno; &#125; public void setDeptno(int deptno) &#123; this.deptno = deptno; &#125;&#125; 分区 什么是分区? partition结合关系型数据库Oracle MR的分区: 根据Map的输出&lt;key2,value2&gt;进行分区 默认的情况下 MR只有一个分区 (一个分区就是一个文件) 自定义分区 : 按照员工的部门号进行分区 合并 Combiner 是一种特殊的Reducer 合并实在Map端执行一次合并 用于减少Mapper输出到Redeucer的数据量 可以提高效率 MapReduce核心: shuffle(洗牌) Hadoop 3.x 以前: 会有数据落地(产生I/O操作) 实际案例1 数据去重 复习SQL: distinct实现去重 作用于后面所有的列 一个列 select job from emp select distinct job form emp多个列 select distinct deptno,job from emp 使用MapReduce实现去重 关系型数据库中的多表查询(子查询: 在Oracle中 绝大部分的子查询 都是转换成多表查询的)(1) 笛卡尔积: 列数相加 行数相乘(2) 根据链接条件的不同 等值连接 不等值链接 外链接 内连接 多表查询 等值连接 3.多表查询 自连接在Oracle 中 当查询的数据满足一棵树的时候 可以使用层次查询来取代自连接 倒排索引 框架MRunit: 单元测试 MapReduce]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作HDFS]]></title>
    <url>%2F2019%2F03%2F04%2F%E6%93%8D%E4%BD%9CHDFS%2F</url>
    <content type="text"><![CDATA[操作HDFS Web Console : 端口50070 命令行: 类似Linux 普通的操作命令: hdfs dfs (另一种写法 : hadoop fs ) -mkdir 查看目录 -ls -ls -R 查看目录和子目录，另一种写法：-lsr 上传数据 -put -copyFromLocal -moveFromLocal: 相当于ctrl+X（剪切） 下载数据 -get -copyToLocal -rm：删除 参数： -rmr 表示删除目录和子目录 hdfs dfs -rmr /output 日志：Deleted /output 注意：如果启用回收站，观察日志 -getmerge：把某个HDFS的目录下的文件，先合并，再下载 举例：（students的数据，就是Hive的外部表用的数据） [root@bigdata ~]# hdfs dfs -mkdir /students [root@bigdata ~]# vi student01.txt [root@bigdata ~]# vi student02.txt [root@bigdata ~]# hdfs dfs -put student0*.txt /students [root@bigdata ~]# hdfs dfs -ls /students Found 2 items -rw-r--r-- 1 root supergroup 19 2018-09-04 04:46 /students/student01.txt -rw-r--r-- 1 root supergroup 10 2018-09-04 04:46 /students/student02.txt [root@bigdata ~]# hdfs dfs -getmerge /students /root/a.txt [root@bigdata ~]# more a.txt 1,Tom,23 2,Mary,26 3,Mike,24 -cp -mv -count：统计HDFS目录下目录个数、文件个数、文件总的大小 [root@bigdata ~]# hdfs dfs -count /students 1 2 29 /students -du: hdfs dfs -du /students 19 /students/student01.txt 10 /students/student02.txt -text、-cat: 查看文本的内容 HDFS的管理命令: hdfs dfsadmin ** [-report] 打印报告 [-safemode enter | leave | get | wait] 安全模式 enter: 手动进入安全状态 leave: 手动离开安全模式 get: 获取安全模式状态 wait: Java API依赖的jar包： $HADOOP_HOME/share/hadoop/common/*.jar $HADOOP_HOME/share/hadoop/common/lib/*.jar $HADOOP_HOME/share/hadoop/hdfs/*.jar $HADOOP_HOME/share/hadoop/hdfs/lib/*.jar 创建目录 1234567891011121314151617@Testpublic void mkDir() throws IOException &#123; //指定当前的Hadoop用户 System.setProperty("HADOOP_USER_NAME","root"); Configuration conf = new Configuration(); conf.set("fs.defaultFS","hdfs://192.168.248.136:9000"); FileSystem client = FileSystem.get(conf); client.mkdirs(new Path("/filetest")); client.close();&#125; 上传数据 下载数据 12345678910111213141516171819202122232425262728293031323334@Testpublic void updateData() throws IOException &#123; Configuration conf = new Configuration(); conf.set("fs.defaultFS","hdfs://192.168.248.136:9000"); FileSystem client = FileSystem.get(conf); //构造一个输入流 从本地读入数据 InputStream inputStream = new FileInputStream("d:\\1.txt"); //构造一个输出流 指向HDFS OutputStream outputStream = client.create(new Path("/filetest/atest.txt")); /* //构造一个缓冲区 byte[] buffer = new byte[1024]; //长度 int len = 0 ; while ((len = inputStream.read(buffer))&gt;0)&#123; //写入到输出流 outputStream.write(buffer,0,len); &#125; //刷新 outputStream.flush(); //关闭流 inputStream.close(); outputStream.close();*/ IOUtils.copyBytes(inputStream,outputStream,1024);&#125; 查看文件的信息 数据节点信息 HDFS的原理分析 建立RPC通信remote proceduer call 远程过程调用(协议) HDFS的高级特性 安全模式 safe mode 注意: HDFS正常运行的时候 安全模式是off关闭的状态 是HDFS的一种自我保护 检查数据块的副本率 如果HDFS处于安全模式 是只读的状态 集群启动的时候 会进入到安全模式 当系统处于安全模式时 会检查数据块的完整性 加入在dfs.replication中定义的是5 那么在一个datanode上 就应该有五个副本存在 假设只有三个副本那么比例就是3/5=0.6,在配置文件hdfs-default中定义了一个最小的副本率为0.99,因为0.6小雨0.99,系统会自动的复制副本到其他的dataNode. 快照: 是一种备份 [-allowSnapshot &lt;snapshotDir&gt;] [-disallowSnapshot &lt;snapshotDir&gt;] 操作命令 -createSnapshot 目录 快照名称 快照的本质: 就是复制文件到一个隐藏目录 (1) 启用目录的快照功能eg. hdfs dfsadmin -allowSnapshot /input hdfs dfs -createSnapshot /input backup_input_201803_05 配额 Quota 名称配额: 限定HDFS目录下 存放文件的个数 [-setQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;] [-clrQuota &lt;dirname&gt;...&lt;dirname&gt;] eg. hdfs dfs -mkdir /myquta规定最多可以存放三个文件(实际存放n-1) hdfs dfsadmin -setQuota 3 /myquota 空间配额 限定HDFS目录下 文件的大小 [-setSpaceQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;] [-clrSpaceQuota &lt;dirname&gt;...&lt;dirname&gt;] 不管文件有多大 hadoop都是按照128M来存储的 所以设置的值 是不可以小于数据块大小的 hdfs dfsadmin -setSpaceQuota 300M /myquota 回收站: 默认是禁用的 回收站默认是关闭的 可以通过在core-site.xml中添加fs.trash.interval来打开配置时间 1234&lt;property&gt;&lt;name&gt;fs.trash.interval&lt;/name&gt;&lt;value&gt;1440&lt;/value&gt;&lt;/property&gt; 删除文件时 其实是放入回收站 /trash 回收站的本质就是把一个目录 移动到了隐藏目录下 回收站里的文件可以快速恢复 Oracle数据库也有回收站 1flashback table (表名) to before drop 安全性 Kerberos HDFS的底层原理 HDFS的底层通信原理采用的是: RPC和动态代理对象Proxy1 RPC: remote proceduer call 远程调用2 Java的动态代理对象 如果一个类的名字前有$ 表示是一个代理对象 是一种包装设计模式 可以增强类的功能 数据库连接池]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS的体系结构]]></title>
    <url>%2F2019%2F03%2F04%2FHDFS%E7%9A%84%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[HDFS的体系结构 NameNode: 名称节点 职责 HDFS的主节点 管理员 接收客户端(命令行和Java)的请求,穿件目录 上传数据 下载数据 删除数据 管理和维护HDFS的日志和元信息 日志文件(edits文件): 记录的是客户端的所有操作 体现了HDFS最新的状态 是一个二进制文件 位置: tmp/dfs/name/current edits_inprogress_0000000000000000118 正在操作的日志信息 HDFS提供了一个日志查看器(edits viewer) hdfs oev -i edits_inprogress_0000000000000000118 -o ~/a.xml (日志文件的转换) 元信息文件(fsimage文件): 记录的是数据块的位置信息. 数据块的冗余信息是一个二进制文件位置tmp/dfs/name/currentHDFS 提供了元信息查看器(image viewer) 把fsimage文件转换为xml文件hdfs oiv -i fsimage_000000000000000 DataNode: 数据节点 按照数据块 保存数据库1.x:64M 2.x:128M 数据块 表现形式:就是一个文件/root/training/hadoop/tmp/dfs/data/current/BP-141479076-127.0.0.1-1551409103135/current/finalized 设置数据块冗余度原则: 一般跟数据节点的个数一样 但是最大不要超过3 Hadoop 3.x以前 会造成存储空间的极大浪费Hadoop 3.x之后 HDFS 纠删码技术 大大的节约存储的空间(节约一半的存储空间) SecondaryNameNode: 第二名称节点职责 进行日志信息的合并 由于edits文件记录了最新的状态信息,并且随着操作越多 edits就越大 把edits中的最新信息写到fsimage中 日志文件可以清空 补充点知识: 检查点 checkpoint Spark中RDD的检查点: 容错机制 Oracle中的检查点: 会以最高优先级唤醒数据库的写进程 将脏数据写入硬盘文件]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop分布式搭建]]></title>
    <url>%2F2019%2F03%2F01%2Fhadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[分布式大数据环境 至少需要三台Linux的机器 准备工作 安装三台Linudx 机器 JDK 并关闭防火墙 设置主机名和IP 配置免密登录 需要两两之间的免密登录ssh-keygen -t rsassh-copy-id -i .ssh/id_rsa.pub root@bigdata1ssh-copy-id -i .ssh/id_rsa.pub root@bigdata2ssh-copy-id -i .ssh/id_rsa.pub root@bigdata3 要保证集群时间的同步 在主节点上安装(bigdata1) hadoop-env.shexport JAVA_HOME=/home/jdk8 hdfs-site.xml 12345678910 &lt;configuration&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;2&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.permissions&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; core-site.xml 123456789101112&lt;!--配置HDFS主节点的地址，就是NameNode的地址--&gt; &lt;!--9000是RPC通信的端口--&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://192.168.248.134:9000&lt;/value&gt; &lt;/property&gt; &lt;!--HDFS数据块和元信息保存在操作系统的目录位置--&gt; &lt;!--默认是Linux的tmp目录,一定要修改--&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/root/training/hadoop/tmp&lt;/value&gt; &lt;/property&gt; mapred-site.xml（默认没有这个文件） 12345&lt;!--MR程序运行容器或者&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; yarn-site.xml 1234567891011&lt;!--配置Yarn主节点的位置--&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;192.168.248.134&lt;/value&gt;&lt;/property&gt; &lt;!--NodeManager执行MR任务的方式是Shuffle洗牌--&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt; slaves 从节点的地址 12192.168.248.128192.168.248.129 对namenode 格式化 1hdfs namenode format 把主节点安装好的目录复制到从节点上scp -r hadoop/ root@bigdata2:/root/training/scp -r hadoop/ root@bigdata3:/root/training/ 在主节点上 start-all.sh]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop的起源与背景]]></title>
    <url>%2F2019%2F02%2F27%2Fhadoop%E7%9A%84%E8%B5%B7%E6%BA%90%E4%B8%8E%E8%83%8C%E6%99%AF%2F</url>
    <content type="text"><![CDATA[什么是大数据 电商的推荐系统: 大量的订单如何存储 大量的订单进行计算(使用一些推荐的算法 协同过滤 ALS 逻辑回归) 天气的预报: 大量的天气数据如何存储 大量的天气数据如何进行计算 核心的问题: 数据的存储—————- 分布式的文件系统: HDFS 数据的计算—————- 分布式的计算:MapReduce Spark(RDD 弹性分布式数据集) 数据仓库和大数据 传统方式: 搭建数据仓库(Data Warehouse)来解决大数据的问题 数据仓库就是一个数据库, 一般只做查询 大数据也是一般只做查询(分析) 搭建数据仓库的过程 Hadoop Spark的一些组件 OLTP 和 OLAP OLTP: Online Transaction Processing 联机事务处理 指 insert uptdate delete —-&gt; 事务传统的关系型数据库解决的问题 OLAP Online Analytic Processing 联机分析处理 一般只做查询 分析数据仓库 就是一种OLAP的应用系统Haddop Spark 看成一种数据仓库的解决方案 Google的基本思想: 三篇论文 GFS: Google File System1.分布式文件系统2.大数据的存储问题3.HDFS中 ,记录数据保存的位置信息 (元信息)——-&gt; 采用倒排索引(Reverted Index) 什么是索引? index(1) create index 创建索引create index myindex on emp(deptno) 根据部门id 查询用户信息.(2)就是一个目录 正常的数据是无序的存放的 使用索引可以存放他们的地址信息 让这些数据按照某种顺序排列 可以加快查询速度(3)通过索引可以找到对应数据(4)问题: 索引一定可以提高查询的速度吗 什么是倒排索引?通过MapReduce 建立倒排索引列表 MapReduce 分布式计算模型 PageRank(网页排名)分布式计算模型Google的向量矩阵先拆分 再合并Yarn = ResourceManager + Nodemanger Example: /root/training/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.4.1.jar 命令:hadoop jar hadoop-mapreduce-examples-2.4.1.jar wordcount /aaa/data.txt /output/day0228/wcl 出现 系统 32位 64位 不匹配问题http://dl.bintray.com/sequenceiq/sequenceiq-bin/ 下载对应的编译版本 [hadoop@hadoopTest ~]$ tar -xvf hadoop-native-64-2.7.0.tar -C hadoop-2.7.2/lib/native[hadoop@hadoopTest ~]$ tar -xvf hadoop-native-64-2.7.0.tar -C hadoop-2.7.2/lib解压到对应的目录 添加环境变量[hadoop@hadoopTest hadoop-2.7.2]$ vi /etc/profileexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS=”-Djava.library.path=$HADOOP_HOME/lib” 然后source 一下使用hadoop checknative –a 进行自检 BigTable ——NoSQL数据库: HBase1 关系型数据库 Oracle MySql ——&gt; 行式数据库——-&gt; insert update delete关系模型(二维表)的形式来存储数据 2常见的NoSQL数据库 Redis 内存数据库 HBase 面向列 ——&gt;列式数据库 MongoDB 面向文档(BSON文档:是JSON的二进制) 大表的基本思想: 把所有的数据存入一张表: 通过牺牲空间 来换取时间HBase = Zookeeper+HMaster(主节点)+RegionServer(从节点) yum install net-tools ifconfig不好使的时候 没有ip地址]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cas单点登录]]></title>
    <url>%2F2018%2F12%2F21%2FCas%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[单点登录的应用场景是企业有很多个系统,当这些系统在同一个域名下的时候,可以通过cookie的方式来传递用户的登录信息. 也就是用户访问网站后 会校验用户的权限,如果没有权限那么就从cookie中取出用户名密码登录,只是登录的这个操作不是用户来执行的.但是逻辑代码是写在后台的.如果涉及修改操作 那么就要修改 多个页面的登录操作. 就出现了第二代的单点登录方式. 统一认证中心因为cookie的登录认证是比较分散的,每一个网站在用户登陆的时候,都要生成一个cookie来存储你用户的登录信息.生成唯一的认证代码.所以就将认证统一化,形成了一个独立的服务,当用户登录的时候,会重定向到统一认证中心.所以用户登录时候 过滤器判断用户是否登录 没有登录就要重定向到认证中心进行认证.用户在认证中心登陆后,会把用户的登录信息记录到服务器的session中. 然后认证中心会给浏览器发送一个凭证,浏览器通过拿着这个凭证登录时候 登录的界面会根据这个凭着去验证中心验证凭证是否有效.来判断用户是否登录成功. cas单点登录服务端测试在cas中 cas包含着两个部分 一个是cas server 是需要独立部署的 就是认证中心.cas Cilent 是负责处理对客户端瘦保护资源的请求,登录时要重定向到cas server cas 我所理解的原理大概是这样的 系统的部署是用的3.x版本.在近期更新的版本中 官方更推荐于使用maven和gradle来进行项目的获取.需要自己打war包 3.x版本是提供war包的 把war包放到你的tomcat中.修改一下server文件.可能会遇到的一个情况是 你运行的依旧是你之前的tomcat这是因为在start.bat中 有这样的一段代码12345678Guess CATALINA_HOME if not definedset &quot;CURRENT_DIR=%cd%&quot;if not &quot;%CATALINA_HOME%&quot; == &quot;&quot; goto gotHomeset &quot;CATALINA_HOME=%CURRENT_DIR%&quot;if exist &quot;%CATALINA_HOME%\bin\catalina.bat&quot; goto okHomecd ..set &quot;CATALINA_HOME=%cd%&quot;cd &quot;%CURRENT_DIR%&quot; 大概就是判断你的CATALINA_HOME是不是为空,然后给定义你要运营的tomcat的地址.我尝试注释了一下 按理说是应该好用的.但是跑的还是之前的tom,因为主要是想跑起来cas,所以就没有深究.直接去环境变量里把CATALIN_HOME给删掉.Ok 好用了.然后访问’localhost://8080/cat/login’当然具体访问的地址 还是要看你的server中的配置地址.然后可以看到登录界面.可以看到有一个报错,原因是https和http的问题.后边会再解决.然后登录用户名和密码casuser,Mellon登录成功 然后修改端口号的方法.去修改tomcat的service文件和cas中WEB/INF/cs.propertiesd的server.name地址两个地址需要一致. 连接数据库(sqlserver)看了一些网上的资料都是介绍连接mysql的,自己电脑本地数据库用的是sqlserver.所以是按照sqlserver配置的.通过和mysql配置的对比.也可以很快的配置上oracle. 首先是修改tomcat地址/CAS/WEB-INF下的deployerConfigContext.xml文件.这里因为主要是做测试.而且由于子项目的加密方式没有确定和统一 我没有添加加密 只是配置了数据库连接.找到&lt;property name=&quot;authenticationHandlers&quot;&gt; 在list中添加\ 123456&lt;bean id="primaryAuthenticationHandler" class="org.jasig.cas.adaptors.jdbc.QueryDatabaseAuthenticationHandler"&gt; &lt;property name="dataSource" ref="casDataSource" /&gt; &lt;!--SELECT PASSWORD FROM tablename WHERE --&gt; &lt;property name="sql" value="SELECT password FROM tb_user WHERE username =?" /&gt; &lt;/bean&gt; 然后再在xml文件中添加 1234567891011121314 &lt;bean id="casDataSource" class="org.springframework.jdbc.datasource.DriverManagerDataSource"&gt; &lt;property name="driverClassName"&gt; &lt;value&gt;com.microsoft.sqlserver.jdbc.SQLServerDriver&lt;/value&gt; &lt;/property&gt; &lt;property name="url"&gt; &lt;value&gt;jdbc:sqlserver://localhost:1433;DatabaseName=tb_user&lt;/value&gt; &lt;/property&gt; &lt;property name="username"&gt; &lt;value&gt;你自己的数据库用户名&lt;/value&gt; &lt;/property&gt; &lt;property name="password"&gt; &lt;value&gt;登录密码&lt;/value&gt; &lt;/property&gt; &lt;/bean&gt; 然后在Cas的lib目录中,添加连接数据库需要的jar包. 数据库建立对应的表,填入数据 然后重启tomacat,可以使用数据库的用户名和账号登录.如果出现类似于not found之类的错误 请检查jar包版本 和jar包是否加入到了项目的lib中,而不是tomcat的lib中. 修改默认登录页.cas 登录页是在 CAS/WEB-INF/view/jsp/default/ui中的,includes中的top.jsp和buttom.jsp是所有页面的头部和底部.casLoginView.jsp是登录界面.如果觉得接下来的xml配置比较麻烦.可以备份后直接,直接在casLoginView.jsp上修改需要的内容 如果要自己重新配置新的页面,可以在cas/WEB-INF/view/jsp目录中将default复制,并修改名字,这里我举例为test文件夹然后在WEB-INF/classes中,找到default_view.properties复制一份.粘贴改名为test_view.properties(这个命名的文件后边会用到)然后打开test_view.properties文件,把里边的/default/目录换成你自己的目录文件.我这里命名的是test所以换为test.然后在cas.properties文件中将cas.viewResolver.basename=default_view修改为cas.viewResolver.basename=test_views(前边提到过的properties文件的名字)修改cas/WEB-INF/classes&#39;中的cas-theme-default.properties文件,将standard.custom.css.file`内容修改为你自己的css路径即可. 与客户端的联调通过maven,新建项目casdemo,配置好对应的端口号pom.xml文件中添加12345678910111213141516171819202122232425262728293031323334353637383940&lt;dependencies&gt; &lt;!-- cas --&gt; &lt;dependency&gt; &lt;groupId&gt;org.jasig.cas.client&lt;/groupId&gt; &lt;artifactId&gt;cas-client-core&lt;/artifactId&gt; &lt;version&gt;3.3.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat8-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;!-- 指定端口 --&gt; &lt;port&gt;8086&lt;/port&gt; &lt;!-- 请求路径 --&gt; &lt;path&gt;/&lt;/path&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; web.xml文件中添加1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd&quot; version=&quot;4.0&quot;&gt; &lt;!-- 用于单点退出，该过滤器用于实现单点登出功能，可选配置 --&gt; &lt;listener&gt; &lt;listener-class&gt;org.jasig.cas.client.session.SingleSignOutHttpSessionListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!-- 该过滤器用于实现单点登出功能，可选配置。 --&gt; &lt;filter&gt; &lt;filter-name&gt;CAS Single Sign Out Filter&lt;/filter-name&gt; &lt;filter-class&gt;org.jasig.cas.client.session.SingleSignOutFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CAS Single Sign Out Filter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;!-- 该过滤器负责用户的认证工作，必须启用它 --&gt; &lt;filter&gt; &lt;filter-name&gt;CASFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.jasig.cas.client.authentication.AuthenticationFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;casServerLoginUrl&lt;/param-name&gt; &lt;param-value&gt;http://localhost:8009/cas/login&lt;/param-value&gt; &lt;!--这里的server是服务端的IP --&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;serverName&lt;/param-name&gt; &lt;param-value&gt;http://localhost:8086&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CASFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;!-- 该过滤器负责对Ticket的校验工作，必须启用它 --&gt; &lt;filter&gt; &lt;filter-name&gt;CAS Validation Filter&lt;/filter-name&gt; &lt;filter-class&gt; org.jasig.cas.client.validation.Cas20ProxyReceivingTicketValidationFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;casServerUrlPrefix&lt;/param-name&gt; &lt;param-value&gt;http://localhost:8009/cas&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;serverName&lt;/param-name&gt; &lt;param-value&gt;http://localhost:8086&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CAS Validation Filter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;!-- 该过滤器负责实现HttpServletRequest请求的包裹， 比如允许开发者通过HttpServletRequest的getRemoteUser()方法获得SSO登录用户的登录名，可选配置。 --&gt; &lt;filter&gt; &lt;filter-name&gt;CAS HttpServletRequest Wrapper Filter&lt;/filter-name&gt; &lt;filter-class&gt; org.jasig.cas.client.util.HttpServletRequestWrapperFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CAS HttpServletRequest Wrapper Filter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;!-- 该过滤器使得开发者可以通过org.jasig.cas.client.util.AssertionHolder来获取用户的登录名。 比如AssertionHolder.getAssertion().getPrincipal().getName()。 --&gt; &lt;filter&gt; &lt;filter-name&gt;CAS Assertion Thread Local Filter&lt;/filter-name&gt; &lt;filter-class&gt;org.jasig.cas.client.util.AssertionThreadLocalFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CAS Assertion Thread Local Filter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt;&lt;/web-app&gt; 新建indexxml文件中 8009是我服务端地址,8086是demo地址.改成对应的就行了启动项目,访问8086会跳转到8009 登陆后会跳转到8086的index界面.测试完成.后面会考虑到关于自定义登录和登录返回值的问题.]]></content>
      <categories>
        <category>单点登录</category>
      </categories>
      <tags>
        <tag>单点登录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot聚合项目配置多数据源]]></title>
    <url>%2F2018%2F11%2F30%2Fspringboot%E8%81%9A%E5%90%88%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%2F</url>
    <content type="text"><![CDATA[在搭建了项目之后,就涉及到了连接数据库的问题,因为考虑到了以后项目可能会使用到多数据源.所以虽然目前只用了一个数据库 但是按照多数据源配置的. 数据源的配置其实并没有什么大的问题,主要的问题还是卡在了在dao层做测试 测试环境的问题 记录一下问题和解决的办法 其实多数据源的配置是比较简单的,在你的.yml文件或者我是’.properties’文件中.需要对你的不同数据源起一个不同的名字,然后在你的工具类中配置好就可以.具体的代码如下 我使用的是.yml文件. 用的是sqlserver的数据库.(实在是不想用oracle=.=)1234567spring: datasource: master(你的数据源名字): driver-class-name: com.microsoft.sqlserver.jdbc.SQLServerDriver url: jdbc:sqlserver://localhost:1433;DatabaseName=`你的数据库名称` username: 用户名 password: 用户密码 然后我是用了三个类123456789101112131415161718192021222324252627282930313233343536373839404142@Component//这个写的就是你写在yml文件或者properties文件中的前缀.@ConfigurationProperties(prefix = "spring.datasource.master")public class MasterDataSourceProperties &#123; private String driverClassName; private String url; private String username; private String password; public String getDriverClassName() &#123; return driverClassName; &#125; public void setDriverClassName(String driverClassName) &#123; this.driverClassName = driverClassName; &#125; public String getUrl() &#123; return url; &#125; public void setUrl(String url) &#123; this.url = url; &#125; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Configuration@MapperScan(basePackages = &#123;MasterDataSourceConfig.PACKAGE1&#125;, sqlSessionFactoryRef = MasterDataSourceConfig.NAME + "SqlSessionFactory")public class MasterDataSourceConfig &#123; @Resource private MasterDataSourceProperties masterDataSourceProperties; // 精确到 master 目录，以便跟其他数据源隔离 //dao目录 static final String PACKAGE1 = "com.emp.mapper"; //xml目录 private static final String mapperLocation1 = "classpath:com/emp/mapper/*.xml"; private static final String[] mapperLocations = &#123;mapperLocation1&#125;; //全局名字前缀 static final String NAME = "master"; //数据源 @Bean(name = NAME + "DataSource") @Primary public DataSource dataSource() &#123; return MyUtils.getDruidDataSource( masterDataSourceProperties.getDriverClassName(), masterDataSourceProperties.getUrl(), masterDataSourceProperties.getUsername(), masterDataSourceProperties.getPassword()); &#125; //事务管理器 @Bean(name = NAME + "TransactionManager") @Primary public PlatformTransactionManager transactionManager() &#123; return new DataSourceTransactionManager(dataSource()); &#125; //工厂 @Bean(name = NAME + "SqlSessionFactory") @Primary public SqlSessionFactory sqlSessionFactory(@Qualifier(NAME + "DataSource") DataSource dataSource) throws Exception &#123; final SqlSessionFactoryBean sessionFactory = new SqlSessionFactoryBean(); sessionFactory.setDataSource(dataSource); sessionFactory.setMapperLocations(MyUtils.resolveMapperLocations(mapperLocations)); return sessionFactory.getObject(); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445public static DruidDataSource getDruidDataSource(String driverClassName, String url, String username, String password) &#123; DruidDataSource dataSource = new DruidDataSource(); //这一项可配可不配，如果不配置druid会根据url自动识别dbType，然后选择相应的driverClassName dataSource.setDriverClassName(driverClassName); //连接数据库的url dataSource.setUrl(url); //连接数据库的用户名 dataSource.setUsername(username); //连接数据库的密码 dataSource.setPassword(password); //初始化时建立物理连接的个数。初始化发生在显示调用init方法，或者第一次getConnection时 dataSource.setInitialSize(1); //最小连接池数量 dataSource.setMinIdle(1); //最大连接池数量 dataSource.setMaxActive(20); //获取连接时最大等待时间，单位毫秒。配置了maxWait之后，缺省启用公平锁，并发效率会有所下降，如果需要可以通过配置useUnfairLock属性为true使用非公平锁。 dataSource.setMaxWait(1000); return dataSource; &#125; /** * org.mybatis.spring.boot.autoconfigure包下MybatisProperties里面的方法直接拿来用 * * @param mapperLocations xml路径数组 * @return 资源数组 */ public static Resource[] resolveMapperLocations(String[] mapperLocations) &#123; ResourcePatternResolver resourceResolver = new PathMatchingResourcePatternResolver(); List&lt;Resource&gt; resources = new ArrayList(); if (mapperLocations != null) &#123; String[] var3 = mapperLocations; int var4 = var3.length; for (int var5 = 0; var5 &lt; var4; ++var5) &#123; String mapperLocation = var3[var5]; try &#123; Resource[] mappers = resourceResolver.getResources(mapperLocation); resources.addAll(Arrays.asList(mappers)); &#125; catch (IOException var8) &#123; ; &#125; &#125; &#125; return resources.toArray(new Resource[resources.size()]); &#125; 代码直接粘 修改一下其中对应的路径就可以了.如果想仔细研究的话可以看看代码,这样其实就算是ok了 但是因为测试的时候也不能直接通过controller测,所以自己想在test环境下测试一下.就出现了几个问题 第一个 空指针 这个问题我是找了挺长时间 也没发现问题原因 后来无意一撇发现聚合项目的test环境跟单项目的test环境是不一样的 注释不全所以在加上了 springboottest和runwith的注释后就不报空指针的错误了. 第二个 test环境版本和正式环境版本不同 这个报错具体内容记不太清了 在修改了版本之后就好了 所以测试的时候要注意一下通过maven引入的jar包版本. 第三个 依赖application 报错是会提示你没有依赖application 在test环境下@springboottest后 加上(classes = ‘你的application名字’.class)就可以了 整个测试环境就这样是可以跑起来的 后来又用了swagger2 作为接口的调试.swagger2 在你的application的统计目录下 建一个类 写一下配置就可以了12345678910111213141516171819202122232425262728/** * @Description:swagger2的配置文件，这里可以配置swagger2的一些基本的内容，比如扫描的包等等 */@Beanpublic Docket createRestApi() &#123; return new Docket(DocumentationType.SWAGGER_2).apiInfo(apiInfo()) .select() .apis(RequestHandlerSelectors.basePackage("com.emp.controller")) .paths(PathSelectors.any()) .build();&#125;/** * @Description: 构建 api文档的信息 */private ApiInfo apiInfo() &#123; return new ApiInfoBuilder() // 设置页面标题 .title("follower后端api接口文档") // 设置联系人 .contact(new Contact("employeeeee", "https://www.zhouzihao.xyz", "employeeeee@sina.com")) // 描述 .description("欢迎访问follower接口文档，这里是描述信息") // 定义版本号 .version("1.0").build();&#125; 然后需要对你的controller 和 model层做@api的注释. 就不详细描述了.项目跑起来之后 本地项目端口/swagger-ui.html就可以对你的接口进行测试了]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot maven idea</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用idea搭建一个聚合项目]]></title>
    <url>%2F2018%2F11%2F28%2F%E4%BD%BF%E7%94%A8idea%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%81%9A%E5%90%88%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[一直都是在用idea搭建单项目的工程,一方面是项目需要也是想要学习一下,于是用idea搭建一个聚合的项目.idea搭聚合项目跟eclipse还是有一些区别的,使用的过程中也是遇到了不少坑,解决了之后记录一下,希望对刚想搭建聚合项目的朋友有所帮助. 项目主要的层次为common-pojo-mapper-service-api这样的形式的. 首先先新建一个新的项目 用来存放你所有的子项目 建立项目时候选择Maven 然后不需要使用模板 直接next 就可以 GroupId和’ArtifactId’完全是按照个人喜好来写的,填写好之后,直接next 这里是可以根据你上一步填写的ArtfactId自动生成的 可以修改,注意一些这些名字要一致就可以了,然后点击finish 就可以了. 然后你的项目中出现第一个项目 也就是你的父项目,主要的作用其实就是一个管理和存储其他项目的空间. 然后选中你建立好的项目,右键选择Module,来建立你的子项目 和之前的一样 选择maven 不需要选择模板 然后点击next 会进入到这个界面 Parent会自动选择为你当前选择的项目,当然不放心的话你也可以再点击选择一下.GroupId是和你parent的groupid是一样的, AtifactId 写一下你子项目的名称 前边也提到了我们的结构 所以这个项目起名叫做common 然后下一步 finish即可. 然后在生成的pom.xml文件中,导入你要使用的maven,这个还是根据自己实际项目使用来决定的.这里的pom文件可以把你要用到的所有包通过maven都导入进来, 因为你之后的子项目都是可以继承这些包的. 然后再建立第二个子项目,跟前边的common一样,建立好后,进入设置 点击选择common 就可以看见你导入的包中有了common的文件夹. 之后的和前边的也没有什么特别多的不同,一直建立到api这个项目,因为api项目是要向web提供接口的, 也就是说是最后一个子项目,那么就需要使它运行起来. 在api项目中新建一个类Application(名字是可以自己起的,主要的是这个类的作用) 然后在这个类中加入注释@SptingBootApplication 个人具体代码如下 1234567@SpringBootApplicationpublic class FollowerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(FollowerApplication.class,args); &#125;&#125; 然后为了测试 写一个controller12345678@RestControllerpublic class HelloWorldController &#123; @RequestMapping("/hello") public String Hello()&#123; return "Hello world"; &#125;&#125; 在resource文件下,新建一个yml文件或者properties‘文件’配置一下端口号 因为返回的不是页面而是一个字符串,所以用的是@RestController 然后 之后 再运行你刚才写的Application 输入你的地址就能看见你的hello world了]]></content>
      <categories>
        <category>idea相关</category>
      </categories>
      <tags>
        <tag>idea maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于a标签实效的一些原因和解决办法]]></title>
    <url>%2F2018%2F11%2F23%2F%E5%85%B3%E4%BA%8Ea%E6%A0%87%E7%AD%BE%E5%AE%9E%E6%95%88%E7%9A%84%E4%B8%80%E4%BA%9B%E5%8E%9F%E5%9B%A0%E5%92%8C%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在项目的过程中 web前台出现了一个状态 就是a标签没有效果,不能点击不能跳转.然后找了一下原因 并附上解决办法. 导致&lt;a&gt;标签不能使用的原因是因为,在你放&lt;a&gt;标签的div的上层有其他的div遮挡住了.导致你不能点击到你的&lt;a&gt;标签,所以首先要让自己能 点到&lt;a&gt;标签就可以. 第一种方式 可以通过z-index的方式来解决,把你遮挡住标签的div放到下边,就是在你上层div中添加一条css属性 1z-index:-1 &lt;!--more--&gt; 但是`z-index`属性是必须在position为`absolute` `relative` `fixed`的情况下才可以使用. 而且在我的项目中,我的上级div也是有`&lt;a&gt;`标签的.使用z-index属性之后,上级div中的&apos;&lt;a&gt;&apos;标签页不能使用了. 所以需要修改上级div大小,并修改相应的padding和margin属性. 这个就需要根据实际情况调整.但是是可以保证每个div中的标签都可以使用的.]]></content>
      <categories>
        <category>web相关</category>
      </categories>
      <tags>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssm数据库列名有下划线导致查询名称为空]]></title>
    <url>%2F2018%2F11%2F20%2Fssm%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9C%89%E5%80%BC%E5%90%8E%E5%8F%B0%E5%8F%96%E5%80%BC%E4%B8%BA%E7%A9%BA%2F</url>
    <content type="text"><![CDATA[今天在做项目是时候发现了一个问题数据库可以查询到数据 但是在后台获取的是null上网查了一下原因 最后发现是因为mybatis.xml配置的问题可以自己新建一个xml文件 加入123456&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot;&quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt; &lt;/configuration 然后在其中加入配置如下123456789101112131415161718&lt;configuration&gt; &lt;settings&gt;&lt;!-- 使用列标签代替列名 --&gt;&lt;!-- &lt;setting name=&quot;useColumnLabel&quot; value=&quot;true&quot;/&gt; --&gt; &lt;!--允许 JDBC 支持自动生成主键--&gt; &lt;setting name=&quot;useGeneratedKeys&quot; value=&quot;false&quot;/&gt; &lt;!--是否开启自动驼峰命名规则（camel case）映射，即从经典数据库列名 A_COLUMN 到经典 Java 属性名 aColumn 的类似映射。 --&gt; &lt;setting name=&quot;mapUnderscoreToCamelCase&quot; value=&quot;true&quot;/&gt;&lt;/settings&gt;&lt;/configuration&gt;--------------------- 作者：Cadence_D 来源：CSDN 版权声明：本文为博主原创文章，转载请附上博文链接！ 原文：https://blog.csdn.net/qq_35893120/article/details/78530528]]></content>
      <categories>
        <category>ssm相关</category>
      </categories>
      <tags>
        <tag>ssm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ajax和mybatis多表查询的配合使用]]></title>
    <url>%2F2018%2F11%2F13%2Fajax%E5%92%8Cmybatis%E5%A4%9A%E8%A1%A8%E6%9F%A5%E8%AF%A2%E7%9A%84%E9%85%8D%E5%90%88%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[之前在测试了 一对一的方式后又测试了一下一对多的方式,具体的关系为.一个班级有一个老师有很多学生.在数据库中新建学生表 并存入数据.123456789101112create table orm_student(s_id int identity(14301101,1) primary key ,s_name varchar(20),class_id int) insert into orm_student values('张三',1401)insert into orm_student values('李四',1401)insert into orm_student values('赵武',1401)insert into orm_student values('刘能',1401)insert into orm_student values('王大麻',1401)insert into orm_student values('王心',1402) 然后写好对应的实体类就行了,因为是查询class来搜索到所有学生的.所以在测试时没有写学生相关的mapper文件等.并且要在class的实体类中,添加一条1private List&lt;Student&gt; studentList; 用来查询学生的信息.然后在class的mapper文件中进行配置 具体内容如下.123456789101112131415&lt;resultMap id="BaseResultMap" type="com.example.memory.pc.classes.domain.Classes"&gt; &lt;id column="c_id" property="cId" jdbcType="INTEGER"/&gt; &lt;result column="c_name" property="cName" jdbcType="NVARCHAR"/&gt; &lt;!-- &lt;result column="t_id" property="tId" jdbcType="INTEGER" /&gt;--&gt; &lt;association property="teacher" javaType="com.example.memory.pc.teacher.domain.Teacher"&gt; &lt;id property="tId" column="t_id"/&gt; &lt;result property="tName" column="t_name"/&gt; &lt;/association&gt; &lt;collection property="studentList" ofType="com.example.memory.pc.student.domain.Student"&gt; &lt;id property="id" column="s_id"/&gt; &lt;result property="studentName" column="s_name"/&gt; &lt;/collection&gt; &lt;/resultMap&gt; 其实就是在resultmap中加入学生的属性.因为是list所以配置的时候标签是 然后配置一下ofType就可以了 查询的sql语句如下 1234 selectt2.c_name,t1.t_name,t3.s_namefrom orm_teacher t1 ,orm_class t2,orm_student t3where t1.t_id = t2.t_id and t2.c_id=#&#123;cId&#125;; 在测试环境中可以查询到数据,因为查询的student是list的类型,那么我在把查询出来的整个list作为json数据传给前台的话就有一个解析list中学生list的问题, 所以又测试了一下解析的办法. 在controller中 将数据作为json类型传给前台.1234567891011121314151617@GetMapping("all")private void getAll(Model model, HttpServletResponse httpServletResponse, HttpServletRequest httpServletRequest) throws Exception &#123; String a = httpServletRequest.getParameter("cid"); System.out.println(a); Integer cId = Integer.valueOf(a); /*解决前台json中文乱码问题*/ httpServletResponse.setContentType("text/html;charset=utf-8"); ObjectMapper objectMapper =new ObjectMapper(); String json = objectMapper.writeValueAsString(classesService.getAll(cId)); System.out.println(json); Writer writer = httpServletResponse.getWriter(); writer.write(json); writer.close();&#125; 前台解析json1234567891011121314151617181920212223 $.getJSON('/classes/all',&#123;"cid": a&#125;,function(data)&#123; var list = new Array(); var str=""; for (var x=0; x &lt; data.length;x++ ) &#123; for (var i=0;i&lt;data[x].studentList.length;i++)&#123; console.log(data[x].studentList[i].studentName); console.log(data[x]); /* var str="";*/ str+="&lt;tr&gt;&lt;td&gt;"+1111+"&lt;/td&gt;&lt;td&gt;"+data[x].studentList[i].studentName+"&lt;/td&gt;&lt;td&gt;"+data[x].teacher.tName+"&lt;/td&gt;&lt;td&gt;"+data[x].cName+"&lt;/td&gt;&lt;td&gt;"+1401+"&lt;/td&gt;&lt;/tr&gt;" list.push(str); &#125; &#125; $('#tbody').html(str);/* console.log(list);*/ &#125;) 1234567891011121314151617181920212223&lt;div class="input-group"&gt; &lt;span class="input-group-addon"&gt;@&lt;/span&gt; &lt;input type="text" class="form-control" id="select" placeholder="请输入你要查询的班级id"&gt; &lt;button type="button" class="btn btn-default" onclick="getAll();ajax()" &gt;点击查询&lt;/button&gt;&lt;/div&gt;&lt;div class="col-lg-4"&gt;&lt;/div&gt;&lt;div class="col-lg-4"&gt; &lt;table class="table"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;学生id&lt;/th&gt; &lt;th&gt;学生姓名&lt;/th&gt; &lt;th&gt;指导教师&lt;/th&gt; &lt;th&gt;班级&lt;/th&gt; &lt;th&gt;班级id&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody id="tbody"&gt; &lt;/tbody&gt; &lt;/table&gt;&lt;/div&gt; 前台的html达到效果就是 输入班级id就可以查询到所有的信息了.]]></content>
      <categories>
        <category>ssm相关</category>
      </categories>
      <tags>
        <tag>ajax mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mybatis关联查询-1对1的查询]]></title>
    <url>%2F2018%2F11%2F11%2Fmybatis%E5%85%B3%E8%81%94%E6%9F%A5%E8%AF%A2-1%E5%AF%B91%E7%9A%84%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[因为项目要做地图关联的功能,其中会有关联查询,今天就在自己电脑上先做了一个测试 是学生 老师 班级这样的一个关联查询.先写的是老师和班级的查询 在这里是一个老师对应一个班级 要通过班级的id把这个班级的名称和教师的名称显示出来.在数据库中先建立教师表和班级表.123456--关联查询测试 班级表create table orm_class(c_id integer primary key,c_name NVARCHAR(20),t_id integer) 123456-- 关联查询测试 教师表create table orm_teacher(t_id integer primary key,t_name NVARCHAR(20),) 然后在表中添加数据后 在项目中新建教师和班级的实体类 因为要通过班级查询教师 所以是通过在班级的类中添加了一个 teacher具体代码如下12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.example.memory.pc.classes.domain;import com.example.memory.pc.teacher.domain.Teacher;/** * @author enployeeeee * @Description: 班级类 * @date 2018/11/11 9:58 * @params * @param null */public class Classes &#123; private Integer cId; private String cName; /* * class表中有一个t_id字段 所以在Classes类中定义一个teacher属性 * */ private Teacher teacher; public Integer getcId() &#123; return cId; &#125; public void setcId(Integer cId) &#123; this.cId = cId; &#125; public String getcName() &#123; return cName; &#125; public void setcName(String cName) &#123; this.cName = cName; &#125; public Teacher getTeacher() &#123; return teacher; &#125; public void setTeacher(Teacher teacher) &#123; this.teacher = teacher; &#125; @Override public String toString() &#123; return "Classes&#123;" + "cId=" + cId + ", cName='" + cName + '\'' + ", teacher=" + teacher + '&#125;'; &#125;&#125; 然后定义Teacher类123456789101112131415161718192021222324252627282930313233343536/** * @author employeeeee * @Description: 新建教师类 * @date 2018/11/11 9:48 * @params * @param null */public class Teacher &#123; private Integer tId; private String tName; public Integer gettId() &#123; return tId; &#125; public void settId(Integer tId) &#123; this.tId = tId; &#125; public String gettName() &#123; return tName; &#125; public void settName(String tName) &#123; this.tName = tName; &#125; @Override public String toString() &#123; return "Teacher&#123;" + "tId=" + tId + ", tName='" + tName + '\'' + '&#125;'; &#125;&#125; 然后配置classesmapper文件123456789101112131415161718192021222324&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd" &gt;&lt;mapper namespace="com.example.memory.pc.classes.dao.ClassesMapperDao"&gt; &lt;resultMap id="BaseResultMap" type="com.example.memory.pc.classes.domain.Classes"&gt; &lt;id column="c_id" property="cId" jdbcType="INTEGER"/&gt; &lt;result column="c_name" property="cName" jdbcType="NVARCHAR"/&gt; &lt;!-- &lt;result column="t_id" property="tId" jdbcType="INTEGER" /&gt;--&gt; &lt;association property="teacher" javaType="com.example.memory.pc.teacher.domain.Teacher"&gt; &lt;id property="tId" column="t_id"/&gt; &lt;result property="tName" column="t_name"/&gt; &lt;/association&gt; &lt;/resultMap&gt; &lt;select id="selectTeacher" resultMap="BaseResultMap"&gt; select * from orm_teacher t1 ,orm_class t2 where t1.t_id = t2.t_id and t2.c_id=#&#123;cId&#125;; &lt;/select&gt;&lt;/mapper&gt; 主要就是把teacher的对应属性给卸载了resultmap中 然后在test中进行测试 可以进行关联查询在进行配置的时候可能会遇到的问题(1) 使用工具自动生成的时候 是会把t_id也生成的 不知道会不会有影响 我是直接删去的 用teacher来代替(2) 进行测试时出现了 输出为null的情况 当时查了一下发现有很多种说法 但是我自己出现这个问题的原因 是因为第一次写的时候将resultmap写成了resulttype,后边改过来之后 输出就没问题了]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hive的操作流程]]></title>
    <url>%2F2018%2F11%2F09%2FHive%E7%9A%84%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[ajax+springboot+thymeleaf的使用]]></title>
    <url>%2F2018%2F11%2F08%2Fajax-springboot-thymeleaf%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[最近的一个项目中 涉及到了一个功能 是关于地图的数据联动所以就需要用ajax来异步更新数据.之前搭了一个springboot的项目.所以就用在这个项目上进行了一次尝试.记录一下整个的过程 ,希望会对刚学习ajax的同学有所帮助. 设计思路:前台的div中有一个数字 在前台获取到这个id 然后传到后台后台获取到id为这个值得对象.再将这个对象在前台展示出来. 出现的问题和解决办法.在前台的div中写了一个数字1&lt;div id=&quot;demo&quot;&gt;1&lt;/div&gt; 然后在js中通过 document.getEletmentById(&quot;demo&quot;).innerHTML;获取到了div中的值.然后在ajax的data的text标签中,然后后台通过httpServletRequest.getParameter(&quot;text&quot;)这个id,通过之前写过的跟去id获取对象的方法.来获取这个对象,放到list中,在通过遍历写出来.后台先是报了一个错误.因为通过httpServletRequest.getParameter获取到的是一个String值.但是我的id是一个int值,解决的办法是通过”integer.valueof()”将string类型转换成了integer.然后运行时报了一个”null”的错误.开始以为是空指针的错误,后来发现是因为integer.valueof的字符串是空值会报的错.因为不确定是因为转型的原因还是取值问题,所以在转型的语句上边加了一个sout的语句.做了一个输出.并且在前台取值后加了一个alert.然后取消转型 直接sout获取到的值.运行后发现前台没问题.后台输出了一个null后,输出了正常获取到的id.考虑到可能是加载的问题.把ajax写到了一个function中,然后通过按钮触发函数.运行后,发现先输出null,点击按钮输出正常id.这样就找到了问题的原因.是因为页面加载的时候直接就运行了我在controller下写的方法.突然想到,因为ajax是异步加载,那么应该通过一个链接跳转到这个网页,一个链接是用来执行ajax的.于是在controller 中新加了一个getmapping,直接跳转到当前网页,再通过按钮跳转新链接.完成ajax传值的操作.再次运行.返现没有报错,前台取值正常,后台取值正常,但是前台并没有显示出来对象.显示检查了一下each 发现没有写错.然后在后台的model代码前sout在后台通过根据id获取的对象.运行 发现后台是获取到了这个对象的 但是前台还是没有显示.在这个问题上还是纠结了一会的.然后在前台的function中传了一个参数,把参数通过consle.log输出发现了问题.输出的是一个网页.因为在控制层写的String方法,然后return的是一个html.所以就出现了错误.然后将String方法 改成 viod 返回的值改成json字符串.这里请教了一下同事.具体代码如下123456ObjectMapper objectMapper = new ObjectMapper(); String json = objectMapper.writeValueAsString(userService.getById(userid)); Writer writer = httpServletResponse.getWriter(); writer.write(json); writer.close(); 这里是直接给前台返回了一个json类型的值.然后在前台对json数据进行解析.12345var str=&quot;&quot; str+=&quot;&lt;tr&gt;&lt;td&gt;&quot;+data.userid+&quot;&lt;/td&gt;&lt;td&gt;&quot;+data.username+&quot;&lt;/td&gt;&lt;td&gt;&quot;+data.password+&quot;&lt;/td&gt;&lt;td&gt;&quot;+data.nickname+&quot;&lt;/td&gt;&lt;/tr&gt;&quot;; $(&apos;#tbody&apos;).html(str); tbody是表格的id然后执行发现好用 然后吃饭的时候突然想到.我返回的是一个html,前台没有数据是因为我的数据在返回的html中,那么我可以直接返回这个html给这个div.在div直接加载这个只有表格的网页应该也是可以的 吃完饭测试了一下,果然是可行的.这个思路是没问题的.具体的代码也比较简单.所以就不贴了.整个的思路就是这样的. perfect 也感谢我文武兄弟的帮助.]]></content>
      <categories>
        <category>ssm相关</category>
      </categories>
      <tags>
        <tag>ajax ssm springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ajax与ssm前后台传值]]></title>
    <url>%2F2018%2F11%2F07%2Fajax%E4%B8%8Essm%E5%89%8D%E5%90%8E%E5%8F%B0%E4%BC%A0%E5%80%BC%2F</url>
    <content type="text"><![CDATA[公司最近写的项目需要联动 然后要用到ajax的技术,虽然以前在项目中用到过ajax 但是没有很好地理解晚上的时候 仔细的想了一想 ajax的用法 在这里记录一下 因为之前写过echarts 后台传值的时候是通过ajax传的,个人觉得后台传值还是比较简单的.理论上来说,就是通过sql语句在数据库中进行取值后 将取到的值放在list在controller 把 list放到model中 然后通过遍历 取出数值. 然后可以根据实际项目的需要将不同的属性放在不同的数组中,再在对应的位置将内容取出来.因为是做java的 所以在如何前台向后台传值这里还是稍微的卡了一下.我是这样做测验的在前台的div里 写了一句话.通过jsvar a= document.geteletmentById.InnerHtml获取到了这段话的内容.然后想了一下如何将这个值传到后台.ajax 其中是有一个属性type&#39;的 其实就是对应的controller中的getmapping和postmapping 我第一次写的时候就是因为在get请求中执行的ajax 而我在后台把方法写到了postmapping中 所以没有获取到数据. 接下来就是把我取到的值写到ajax 的 data中data[“a”:a]`(测试写的比较着急 不要在意命名)然后在controller中 通过httphttpServletRequest.getprameter(“a”)获取到ajax传回得数值.我在前台和后台分别写了alert 和 sout最后是成功通过ajax 进行了 前后台的数据交互当然了 通常ajax前台的数值还是在post请求中使用的比较多道理还是一样的在此记录一下 以便日后浏览.]]></content>
      <categories>
        <category>ssm相关</category>
      </categories>
      <tags>
        <tag>ajax ssm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive设计思想与架构]]></title>
    <url>%2F2018%2F11%2F06%2FHive%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3%E4%B8%8E%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[Hive是建立在Hadoop上的 是针对MapReduce开发的技术 Hive的组件主要分为两类: 客户端组件和服务端组件. 下面我们对这些组件进行逐一说明.1) 客户端组件CLI: Command Line Interface,命令行接口 最常用的客户端组件就是CLI CLI启动的时候 会同时启动一个Hive副本Client: Client是Hive的客户端 用于连接Hive Server 在启动Client模式的时候 需要指出Hive Server 所在的节点 , 并且在该节点启动Hive Server图4-2 所示的结构图没有写上 Thrift客户端 但是Hive架构的许多客户端接口 都是建立在Thrift客户端上的 包括JDBC 和 ODBC接口Web GUI : Hive 客户端 提供 了一种通过网页访问Hive 所提供的服务的方式 这个接口duiying`Hive的HWI(Hive Web Interface) 组件 使用前要启动HWI服务2) 服务器端组件Driver 组件: 该组件包括Compiler Optimizer 和 Executor 其作用是完成HiiveQL(类SQL)查询语句的词法分析 语法分析 编译 优化及查询计划的生成.生成的查询计划存储在HDFS中 并在随后又MapReduce调用执行.MetaStore组件: 元数据服务组件 这个组件存储Hive的元数据.Hive的元数据存储在关系数据库里 Hive支持的关系数据库有Derby Mysql.Hive中的元数据包括表的名字 表的列和分区及其属性. 表的属性(是否为外部表等) 表数据所在目录 元数据对于Hive十分重要 所以Hive支持吧MEtaStore服务独立出来 安装到远程的服务器集群里从而解耦Hive服务 和 MetaStore服务 保证Hive运行的健壮性.Hive的MetaStore组件是Hive元数据的集中存放地 MetaStore 组件包括两个部分: MetaStore 服务和 后台数据的存储. 后台数据存储的介质就是关系数据库例如Hive 默认的嵌入式磁盘数据库Derby 还有MYsql 数据库 MetaStore服务是建立在后台数据存储介质之上 并且可以和Hive服务进行交互的服务组件默认情况下 MetaStore服务和Hive服务是安装在一起的 运行在同一个进程当中 也可以把MetaStore服务从Hive服务中剥离出来独立安装在一个集群里 Hive远程调用MetaStore服务 我们可以把元数据这一层放到防火墙之后 当客户端访问Hive服务时 就可以连接到元数据这一层 从而提供更好的管理性能和安全保障. 使用远程的MetaStore服务 可以让MetaStore服务和Hive服务运行在不同的进程里 这样既保证了Hive的稳定 有提升了Hive服务的效率Thrift服务 :Thtift是FaceBook开发的一个软件框架 它用来进行可扩展且跨语言服务的开发 Hive继承了该服务 可以让不同的编程语言调用Hive的接口.]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据 Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive和数据库的区别]]></title>
    <url>%2F2018%2F11%2F05%2FHive%E5%92%8C%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[Hive 和 数据库的区别Hive并不是数据库,从结构上看,Hive和数据库除了有类似的查询语言,再无类似之处.数据库可以用在Online的应用中,但是Hive是为了数据仓库而设计的 数据存储位置 Hive是建立在Hadoop上的,所有Hive的数据都是存储在HDFS中的;而数据库可以将数据保存在块设备或者本地文件系统中 查询语言 由于SQL被广泛地应用在数据仓库中 一次针对Hive的特性设计了类SQL的查询语言HQL 熟悉SQL的开发者可以很方便的使用Hive进行开发. 索引 Hive在加载数据的过程中 不会对数据进行任何处理 甚至不会对数据进行扫描 因此也没有对数据中的某些Key建立索引,Hive要访问数据中满足条件的特定值时需要暴力扫描整个数据 因此访问延迟高,但由于MapReduce的引入 Hive可以进行并行访问数据 因此即使没有索引 对于大数据量的访问 Hive仍然可以体现出优势 数据格式 Hive中没有定义专门的数据格式 数据格式可以由用户指定 用户定义数据格式需要指定三个属性: 列分隔符 行分隔符 以及读取文件数据的方法 .在加载过程中不会对数据本省进行修改 知识复制或者移动到相应的HDFS目录中.而数据库会按照一定的组织存储 过程耗时 执行 Hive查询的执行是通过 MapReduce实现 而数据库有自己的执行引擎. Hive不支持对数据的改写和添加 加载时候是确定好的 延迟 MapReduce本身延迟比较高 数据规模小的时候 数据库的执行延迟较低 可扩展性 由于Hive是建立在Hadoop上 因此Hive的可扩展性和Hadoop一直 而数据库的扩展性很有限 数据规模 由于Hive建立在集群 可以利用MapReduce进行并行计算 因此可以支持很大规模的数据10 Hive对硬件要求比较低 而关系型数据库管理系统为了提高线上处理速度 对硬件要求相对较高.]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hive 大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[idea打开项目没有文件夹]]></title>
    <url>%2F2018%2F11%2F05%2Fidea%E6%89%93%E5%BC%80%E9%A1%B9%E7%9B%AE%E6%B2%A1%E6%9C%89%E6%96%87%E4%BB%B6%E5%A4%B9%2F</url>
    <content type="text"><![CDATA[使用idea打开eclipse项目 会出现没有目录的情况解决办法: 关闭idea 找到项目文件下的.idea文件夹将.idea文件夹删除再重新打开项目即可.]]></content>
      <categories>
        <category>idea使用</category>
      </categories>
      <tags>
        <tag>idea</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive相关]]></title>
    <url>%2F2018%2F11%2F04%2FHive%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[Hive 的简介Hive 蜂巢 是一种数据仓库工具 要明确的是Hive知识一种工具 可以用Hive将结构化的数据文件映射为一张数据库表 并提供完整的SQL 查询功能 可以将Sql 语句 转换为MapReduce任务运行所以Hive 本身是不具备存储功能的 要将它和数据库区分开 Hive的来源Hive是Facebook 开发的构建于Hadoop集群之上的数据仓库应用 它提供了 类似于SQL语法的HQL语句作为数据访问接口.这使普通分析人员在应用HADOOP的过程中更加容易学习.至于Facebook为什么使用hadoop和Hive组建数据仓库 大致原因如下: Facebook的数据仓库开始是构建于MySQL之上的,但是随着数据量的增加 查询需要的时间太久了 当数据量达到1TB 时 MySQL后台进程就崩溃了,所以Facebook决定把数据仓库转移到Oracle.这次转移也付出了很多的代价 Oracle虽然可以应对几TB的数据 但是在手机用户点击流数据(每天大约400gb)后 Oracle 也支持永恒不住了 由此要考虑新的数据仓库方案 4.内部人员简历一个并行日志处理系统Cheetah 勉强可以在24小时内处理一天的点击流数据 但是依旧有很多缺点 后来开发人员尝试将日志数据同事载入Cheetah和Hadoop中对比 发现Hadoop在处理大规模数据更有优势,后来就把工作流都转移到了Hadoop上 并基于Hadoop做了很多有价值的分析 为了使大多数人使用Hadoop 开发了Hive Hive 提供了类似于SQL的查询接口 集群存储2.5PB的数据 并且以每天15TB的数据增长 每天提交3000以上的数据 大约处理 55TB的数据]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据 Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于thymeleaf+layout布局的使用方式]]></title>
    <url>%2F2018%2F11%2F04%2F%E5%85%B3%E4%BA%8Ethymeleaf-layout%E5%B8%83%E5%B1%80%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[最近在做项目时候用到了 layout布局的方式,相对方便了很多,记录一下.这个layout的效果是可以把网页的相同部分提取出来,只需要更改核心不一样的地方就可以了.也就是更改后边我们会提到的content部分. 首先建立相同部分的html,这里我命名为layout.html,放在了`templates/layout’文件夹下,这个路径以后是会用到的,以下是我的layout的代码,比较粗糙.但是应该会更好的帮助理解.要提到几个重要的部分 xmlns:th=”http://www.thymeleaf.org&quot; 引入th xmlns:layout=”http://www.w3.org/1999/xhtml&quot; 引入 thymeleaf 你要将不同内容放置的位置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot; xmlns:layout=&quot;http://www.w3.org/1999/xhtml&quot;&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;欢迎登录&lt;/title&gt; &lt;head&gt; &lt;style&gt; h1 &#123; color: yellowgreen; &#125; table, th, td &#123; border: 1px solid sandybrown; &#125; th &#123; background: sandybrown; color: cornsilk; &#125; .row:after &#123; content: &quot;&quot;; clear: both; display: block; &#125; [class*=&quot;col-&quot;] &#123; float: left; padding: 15px; &#125; html &#123; font-family: &quot;Lucida Sans&quot;, sans-serif; &#125; .header &#123; background-color: #9933cc; color: #ffffff; padding: 15px; &#125; .menu ul &#123; list-style-type: none; margin: 0; padding: 0; &#125; .menu li &#123; padding: 8px; margin-bottom: 7px; background-color: #33b5e5; color: #ffffff; box-shadow: 0 1px 3px rgba(0, 0, 0, 0.12), 0 1px 2px rgba(0, 0, 0, 0.24); &#125; .menu li:hover &#123; background-color: #0099cc; &#125; .aside &#123; background-color: #33b5e5; padding: 15px; color: #ffffff; text-align: center; font-size: 14px; box-shadow: 0 1px 3px rgba(0, 0, 0, 0.12), 0 1px 2px rgba(0, 0, 0, 0.24); &#125; .footer &#123; background-color: #0099cc; color: #ffffff; text-align: center; font-size: 12px; padding: 15px; &#125; .right &#123; float: right; &#125; .col-1 &#123; width: 8.33%; &#125; .col-2 &#123; width: 16.66%; &#125; .col-3 &#123; width: 25%; &#125; .col-4 &#123; width: 33.33%; &#125; .col-5 &#123; width: 41.66%; &#125; .col-6 &#123; width: 50%; &#125; .col-7 &#123; width: 58.33%; &#125; .col-8 &#123; width: 66.66%; &#125; .col-9 &#123; width: 75%; &#125; .col-10 &#123; width: 83.33%; &#125; .col-11 &#123; width: 91.66%; &#125; .col-12 &#123; width: 100%; &#125; @media only screen and (max-width: 768px) &#123; /* For mobile phones: */ [class*=&quot;col-&quot;] &#123; width: 100%; &#125; &#125; &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;center&gt; &lt;div class=&quot;header&quot;&gt; &lt;h2&gt;User Information table&lt;/h2&gt; &lt;span th:text=&quot;$&#123;&apos;Hi &apos;+session.loginUser.username&#125;&quot;&gt;Hi, Admin&lt;/span&gt; &lt;/div&gt; &lt;a th:href=&quot;@&#123;/user/index&#125;&quot;&gt;&lt;/a&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-3 menu&quot;&gt; &lt;ul&gt; &lt;li&gt;用户信息管理&lt;/li&gt; &lt;li&gt;学生信息管理&lt;/li&gt; &lt;li&gt;教师信息管理&lt;/li&gt; &lt;li&gt;课程信息管理&lt;/li&gt; &lt;li&gt;周边商品管理&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=&quot;col-5&quot;&gt; //重点部分 &lt;div layout:fragment=&quot;content&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;col-3 right&quot;&gt; &lt;div class=&quot;aside&quot;&gt; &lt;h2&gt;What?&lt;/h2&gt; &lt;p&gt;Chania is a city on the island of Crete.&lt;/p&gt; &lt;h2&gt;Where?&lt;/h2&gt; &lt;p&gt;Crete is a Greek island in the Mediterranean Sea.&lt;/p&gt; &lt;h2&gt;How?&lt;/h2&gt; &lt;p&gt;You can reach Chania airport from all over Europe.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/center&gt; &lt;div class=&quot;footer&quot;&gt; &lt;p&gt;Resize the browser window to see how the content respond to the resizing.@Design by &lt;a href=&quot;www.baidu.com&quot;&gt;empoloyeeeeee&lt;/a&gt; &lt;/p&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 然后建立新的html,这里我建立的是一个简单的添加用户的界面add.html,代码如下要提到的几个重要的部分 xmlns:layout=”http://www.w3.org/1999/xhtml&quot; xmlns:th=”http://www.thymeleaf.org layout:decorator=”layout/layout” 前边我们提到的路径,这个就是你layout.html文件的位置. 设置div content 然后就可以将你需要更改的内容写在这个位置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot; xmlns:layout=&quot;http://www.w3.org/1999/xhtml&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot; layout:decorator=&quot;layout/layout&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;style&gt; input[type=&quot;text&quot;]:focus&#123; background-color: grey; &#125; input#userid &#123; margin-left: 20px; &#125; #add&#123; padding: 10px 40px; border: 5px gray solid; box-shadow: 10px 10px 5px aquamarine; font-family: 华文行楷; font-size: 14px; border-radius: 20px; &#125; input&#123; padding: 10px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;!--&lt;div class=&quot;bs-example&quot; layout:fragment=&quot;content&quot;&gt;--&gt;&lt;div layout:fragment=&quot;content&quot; &gt; &lt;div id=&quot;add&quot;&gt; &lt;form action=&quot;#&quot; th:action=&quot;@&#123;/user/add&#125;&quot; th:object=&quot;$&#123;userList&#125;&quot; method=&quot;post&quot;&gt; &lt;label&gt;id&lt;/label&gt;&lt;input type=&quot;text&quot; th:field=&quot;*&#123;userid&#125;&quot; id=&quot;userid&quot;/&gt;&lt;br&gt; &lt;label&gt;姓名&lt;/label&gt;&lt;input type=&quot;text&quot; th:field=&quot;*&#123;username&#125;&quot;/&gt;&lt;br&gt; &lt;label&gt;密码&lt;/label&gt;&lt;input type=&quot;text&quot; th:field=&quot;*&#123;password&#125;&quot;/&gt;&lt;br&gt; &lt;label&gt;昵称&lt;/label&gt;&lt;input type=&quot;text&quot; th:field=&quot;*&#123;nickname&#125;&quot;/&gt;&lt;br&gt; &lt;input type=&quot;submit&quot; value=&quot;添加&quot;/&gt; &lt;/form&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 之前在写到这的时候就直接跑程序了 ,发现不好用,但是感觉写的没有问题 ,最后找到了问题所在,在meaven中没有配置thmeleaf,所以在meaven中添加如下代码 12345&lt;dependency&gt; &lt;groupId&gt;nz.net.ultraq.thymeleaf&lt;/groupId&gt; &lt;artifactId&gt;thymeleaf-layout-dialect&lt;/artifactId&gt; &lt;version&gt;2.2.2&lt;/version&gt; &lt;/dependency&gt; controller中的写法我就不过多的叙述了 配置好路径 就可以测试一下了 亲测是通的 效果如图]]></content>
      <categories>
        <category>ssm项目相关</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>thymeleaf</tag>
        <tag>meaven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[idea新建类自动生成注释]]></title>
    <url>%2F2018%2F11%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%9D%A5%E6%BA%90%2F</url>
    <content type="text"><![CDATA[打开idea 使用ctrl+alt+s 打开设置。/* Created by 周子淏 on ${DATE} ${TIME}*/讲这个内容复制粘贴到text框内。新建class后 会自动生成注释。]]></content>
      <categories>
        <category>idea的使用</category>
      </categories>
      <tags>
        <tag>idea</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[firstBlog]]></title>
    <url>%2F2018%2F11%2F03%2FFirstBlog%2F</url>
    <content type="text"><![CDATA[这是我的一个博客]]></content>
  </entry>
</search>
